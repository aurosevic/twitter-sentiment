{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets classification using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "from nltk.data import path\n",
    "# append your path for nltk data\n",
    "path.append(\"C:\\\\Users\\\\andri\\\\AppData\\\\Roaming\\\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "X, y2 = [], []\n",
    "with open('./Data/train.csv', 'rt') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    next(reader, None) # Skip header\n",
    "    \n",
    "    for row in reader:      \n",
    "        y2.append(row[1])\n",
    "        X.append(row[2])\n",
    "\n",
    "y_real = []        \n",
    "for i in y2:\n",
    "    y_real.append(int(i))\n",
    "\n",
    "# Making vector y one_hot\n",
    "y = [] # one hot y\n",
    "for i in range(len(y_real)):\n",
    "    if y_real[i] == 0:\n",
    "        y.append([1, 0])\n",
    "    else:\n",
    "        y.append([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of 0 and 1 classes\n",
    "pozitivan, negativan = 0, 0\n",
    "\n",
    "for y_ch in y_real:\n",
    "    if y_ch == 0:\n",
    "        negativan += 1\n",
    "    else:\n",
    "        pozitivan += 1\n",
    "\n",
    "print('br. pozitivnih: ', pozitivan, '\\nbr. negativnih: ', negativan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validtaiton and test set\n",
    "\n",
    "train_len = int((len(X)/20) * 0.6)\n",
    "validatioon_len = int((len(X)/20)*0.2 + train_len)\n",
    "test_len = 2 * validatioon_len\n",
    "\n",
    "X_train = X[:train_len]\n",
    "Y_train = y[:train_len]\n",
    "\n",
    "X_valid = X[train_len:validatioon_len]\n",
    "Y_valid = y[train_len:validatioon_len]\n",
    "\n",
    "X_test = X[validatioon_len:test_len]\n",
    "Y_test = y[validatioon_len:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sample\n",
    "for i in range(3):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences to tokens\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "X_train_sent = []\n",
    "X_valid_sent = []\n",
    "X_test_sent = []\n",
    "\n",
    "def split_to_sent(sent_array, x_array):\n",
    "    for i in range(len(x_array)):\n",
    "        sent_array.append(sent_tokenize(x_array[i]))\n",
    "\n",
    "split_to_sent(X_train_sent, X_train)\n",
    "split_to_sent(X_valid_sent, X_valid)\n",
    "split_to_sent(X_test_sent, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_sent[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences into words\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "X_train_word, X_valid_word, X_test_word = [], [], []\n",
    "\n",
    "# Funkcija za pronalazenje svih pozicija karaktera ch u stringu s\n",
    "def findOccurences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "def clean_data(data_set_to_split, data_set):\n",
    "    \"\"\"\n",
    "    data_set_to_split - set with sentences to split into words\n",
    "    data_set - set with words\n",
    "    \"\"\"\n",
    "    \n",
    "    line = [] # jedan twit\n",
    "    occurences = [] # lista sa pozicijama karaktera '@' u datoj recenici\n",
    "    occurences_and = [] # lista sa pozicijama karaktera '&' u datoj recenici\n",
    "    http_index = [] # lista sa pozicijama podstringa 'http' u datoj recenici\n",
    "    usernames = [] # list of usernames and strings starting with '&' to remove\n",
    "    http_list = [] # list of links to remove\n",
    "    \n",
    "    for x in data_set_to_split:\n",
    "        duzina = len(x)\n",
    "        for i in range(duzina):\n",
    "            string = str(x[i]).strip()\n",
    "            \n",
    "            # Remove usernames and links\n",
    "            occurences = findOccurences(string, '@')\n",
    "            occurences_and = findOccurences(string, '&')\n",
    "            http_index = [m.start() for m in re.finditer('(?=http)', string)]\n",
    "            \n",
    "            if occurences or occurences_and or http_index: # if any of the lists is not empty\n",
    "                if occurences_and:\n",
    "                    for index in occurences_and: # indexes of '&'\n",
    "                        stop_index = string.find(' ' or '\\n', index) # finds the first occurence of ' ' or '\\n'\n",
    "                        char_and = str(string[index:stop_index])\n",
    "                        usernames.append(char_and)\n",
    "                    occurences_and = []\n",
    "                if occurences:\n",
    "                    for index in occurences: # indexes of '@'\n",
    "                        stop_index = string.find(' ' or '\\n', index)\n",
    "                        user_name = str(string[index:stop_index]) # find twitter username: @blah\n",
    "                        usernames.append(user_name)\n",
    "                    occurences = []\n",
    "                if http_index:\n",
    "                    for index in http_index:\n",
    "                        stop_index = string.find(' ' or '\\n', index)\n",
    "                        link = str(string[index:stop_index])\n",
    "                        http_list.append(link)\n",
    "                    http_index = []\n",
    "\n",
    "                for username_link in usernames or http_list:\n",
    "                    if username_link in string:\n",
    "                        string = string.replace(username_link, '')\n",
    "                usernames = []\n",
    "                http_list = []\n",
    "            line.extend(regexp_tokenize(string, \"[\\w']+\"))\n",
    "        data_set.append(line)\n",
    "        line = []    \n",
    "\n",
    "## Train\n",
    "clean_data(X_train_sent, X_train_word)\n",
    "    \n",
    "## Validation\n",
    "clean_data(X_valid_sent, X_valid_word)\n",
    "\n",
    "## Test\n",
    "clean_data(X_test_sent, X_test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_word[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get a list of stopwords for english\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "stopwords_punctuation_list = set(stopword_list).union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and words that doesn't give huge meaning to sentence\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, regexp_tokenize\n",
    "\n",
    "X_final_train = []\n",
    "X_final_valid = []\n",
    "X_final_test = []\n",
    "\n",
    "def token(word_array, final_array):\n",
    "    for x in word_array:\n",
    "        x = [w.lower() for w in x if w not in stopwords_punctuation_list and not w.isdigit() and len(w) > 1 and not w[0].isdigit() and len(w) > 2]\n",
    "        final_array.append(x)\n",
    "        \n",
    "token(X_train_word, X_final_train)\n",
    "token(X_valid_word, X_final_valid)\n",
    "token(X_test_word, X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_train[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postavljanje reci na korene\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter = LancasterStemmer()\n",
    "\n",
    "def trim_to_root(final_array):\n",
    "    for x in final_array:\n",
    "        duzina = len(x)\n",
    "        for i in range(duzina):\n",
    "            x[i] = porter.stem(x[i])\n",
    "\n",
    "trim_to_root(X_final_train)\n",
    "trim_to_root(X_final_valid)\n",
    "trim_to_root(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_train[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodavanje reci iz trening seta u recnik i odredjivanje da li je rec 'pozitivna' ili 'negativna'\n",
    "\n",
    "class_val = 0\n",
    "vrednosti = {}\n",
    "\n",
    "for x in X_final_train:\n",
    "    duzina_x = len(x)\n",
    "    for i in range(duzina_x):\n",
    "        if x[i] not in vrednosti:\n",
    "            vrednosti.setdefault(x[i], 0)\n",
    "        else:\n",
    "            if Y_train[class_val][0] == 1 and Y_train[class_val][1] == 0: # negativan[0,1], [1,0]\n",
    "                vrednosti[x[i]] -= 1\n",
    "            else:\n",
    "                vrednosti[x[i]] += 1\n",
    "    class_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vrednosti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predstavljanje svakog tvita u setu preko urenjenog para [pos, neg] \n",
    "# [pos, neg] - gde je pos broj pozitivnih reci u tvitu, a neg broj negativnih reci u tvitu\n",
    "\n",
    "X_train_cor = [] # X sa koordinatama\n",
    "X_valid_cor = []\n",
    "X_test_cor = []\n",
    "\n",
    "def assign_coord(final_array, cor_array):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for x in final_array:\n",
    "        duzina_x = len(x)\n",
    "        for i in range(duzina_x):\n",
    "            if x[i] not in vrednosti:\n",
    "                pass\n",
    "            elif vrednosti[x[i]] == 0:\n",
    "                pass\n",
    "            elif vrednosti[x[i]] > 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "        cor_array.append([pos, neg])\n",
    "        pos, neg = 0, 0\n",
    "\n",
    "assign_coord(X_final_train, X_train_cor)\n",
    "assign_coord(X_final_valid, X_valid_cor)\n",
    "assign_coord(X_final_test, X_test_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Za trening i validaciju ne uzimaju se tvitovi za koje ne mozemo da odredimo klasu\n",
    "# Brisanje tvita cija je vrednost [0,0], nema ni pozitivnih ni negativnih reci\n",
    "\n",
    "X_train_final, X_valid_final, X_test_final = X_train_cor, [], []\n",
    "\n",
    "def clear_tweet(coord_array, non_zero_array):\n",
    "    for x in coord_array:\n",
    "        if x[0] != 0 and x[1] != 0:\n",
    "            non_zero_array.append(x)\n",
    "\n",
    "clear_tweet(X_valid_cor, X_valid_final)\n",
    "clear_tweet(X_test_cor, X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_valid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algoritam ce proci kroz k-ove iz k_valid i zapamtiti najbolju preciznost u acc_valid i k u K\n",
    "k_valid = (3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15) # skloni parne brojeve za ovaj data set\n",
    "K_valid = 0 # k iz k_valid koje daje najbojle rezultate ce postati K_valid\n",
    "acc_valid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input train vector\n",
    "x1 = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "\n",
    "# input validation/test vector\n",
    "x2 = tf.placeholder(dtype=tf.float32, shape=[2])\n",
    "\n",
    "# number of classes k\n",
    "K = tf.placeholder(dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate L2 norm\n",
    "\n",
    "# Euklidovo rastojanje\n",
    "distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x1, x2)), axis=1)) # 300x1\n",
    "# weighted distance\n",
    "w_distance = 1.0/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = tf.nn.top_k(tf.negative(w_distance), K)\n",
    "\n",
    "k_nn_labels = tf.gather(Y_train, indices) # vraca vektor lepo mapiranih labela sa njihovim indeksima\n",
    "\n",
    "predict = tf.argmax(tf.reduce_sum(k_nn_labels, axis=0), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    for k in k_valid:\n",
    "        \n",
    "        print('Validatioin for k = ', k)\n",
    "        \n",
    "        accuracy = 0.0\n",
    "\n",
    "        for i in range(len(X_valid_final)):\n",
    "\n",
    "            # nasa koju smo nasli\n",
    "            pred_y = sess.run(predict, feed_dict={x1: X_train_final, x2: X_valid_final[i], K:k})\n",
    "\n",
    "            # proveriti da li je pred_y broj\n",
    "            if not pred_y.dtype == 'int64':\n",
    "                pred_y = Y_train[i][tf.reduce_max(Y_train[i], axis=0).eval()]\n",
    "            \n",
    "            # realna klasa\n",
    "            true_y = tf.argmax(Y_valid[i], axis=0).eval() # eval vrati poziciju najveceg elementa u Y_np_valid\n",
    "\n",
    "            match = pred_y == true_y\n",
    "\n",
    "            print(\"[Validation %3d] Prediction: %d, True Class: %d, Match: %d\" % (i, pred_y, true_y, match))\n",
    "            \n",
    "            #print('X_np_train ', X_np_train[i], ' klasa: ', Y_train[i])\n",
    "            #print('shape: ', )\n",
    "            #print('\\ndistanca: \\n', sess.run(distance, feed_dict={x1: X_np_train, x2: X_np_valid[i], K:k}))\n",
    "            #print('\\nw_distanca: \\n', sess.run(w_distance, feed_dict={x1: X_np_train, x2: X_np_valid[i], K:k}))\n",
    "            #print('*'*25)\n",
    "            \n",
    "            if match:\n",
    "                accuracy += 1.0 / len(X_valid_final)\n",
    "\n",
    "            if accuracy > acc_valid:\n",
    "                acc_valid = accuracy\n",
    "                K_valid = k\n",
    "\n",
    "        print('accuracy for k = ', k, ' -> ', accuracy)\n",
    "    print('The best accuracy', acc_valid, ' is with k = ', K_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy with best k from validatin in test set\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(X_test_final)):\n",
    "\n",
    "        # nasa koju smo nasli\n",
    "        pred_y = sess.run(predict, feed_dict={x1: X_train_final, x2: X_test_final[i], K:K_valid})\n",
    "\n",
    "        # proveriti da li je pred_y broj, ako nije postaviti da bude one klase za koju je distance dobio 0 # isdigit()\n",
    "        if not pred_y.dtype == 'int64':\n",
    "            pred_y = Y_train[i][tf.reduce_max(Y_train[i], axis=0).eval()]\n",
    "\n",
    "        # realna klasa\n",
    "        true_y = tf.argmax(Y_train[i], axis=0).eval() # Y_np_test treba da bude one_hot\n",
    "\n",
    "        match = pred_y == true_y\n",
    "\n",
    "        print(\"[Test %3d] Prediction: %d, True Class: %d, Match: %d\" % (i, pred_y, true_y, match))\n",
    "\n",
    "        if match:\n",
    "            accuracy += 1.0 / len(X_test_final)\n",
    "\n",
    "    print('For k = %d accuaracy on:' % (K_valid))\n",
    "    print(' - validation set is ', acc_valid)\n",
    "    print(' - test set is ', accuracy)\n",
    "\n",
    "    print('Difference in accuracy: ', acc_valid - accuracy) if acc_valid > accuracy else print('difference in accuracy: ', accuracy - acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
